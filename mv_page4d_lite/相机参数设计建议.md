# 多视角相机参数设计建议

## 问题诊断

### 当前问题
4个同步视角的相机参数完全相同，这表明：
1. **CameraHead完全冻结**：无法学习区分不同视角
2. **相机token被过度统一**：在aggregator的attention过程中，不同视角的相机token可能被统一
3. **缺少视角特定的转换**：没有可训练的层来学习视角特定的相机参数映射

## 解决方案

### 方案1：添加可训练的View-Specific Shim层（已实现）✅

**在CameraHead中添加view_proj层**：
- 位置：在`token_norm`之前
- 功能：为每个视角学习独立的投影变换
- 初始化：接近恒等变换（小权重）
- 训练：解冻view_proj，冻结trunk

**优点**：
- 保持CameraHead主干冻结（保留预训练知识）
- 允许学习视角特定的映射
- 参数量小（每个视角只有dim_in -> dim_in/4 -> dim_in的MLP）

### 方案2：部分解冻CameraHead（备选）

如果方案1不够，可以：
- 解冻`pose_branch`（最后一层，输出9维参数）
- 或者解冻trunk的最后1-2层
- 保持其他层冻结

### 方案3：相机参数先验约束

在损失函数中添加相机参数先验：
- 利用360/V度的均匀间隔假设
- 在相机参数的旋转部分添加约束
- 强制不同视角的相机参数有不同的旋转角度

### 方案4：检查相机token的差异性

添加调试代码检查：
1. **提取时的差异**：从aggregator提取相机token时是否还有差异
2. **归一化后的差异**：LayerNorm是否减弱了差异
3. **Shim层后的差异**：view_proj是否增强了差异
4. **最终输出的差异**：CameraHead输出是否不同

## 当前实现状态

### 已实现 ✅
1. **view_proj Shim层**：为每个视角（最多32个）添加独立的MLP投影层
2. **调试输出**：在关键位置添加差异检查
3. **训练配置**：view_proj层被设置为可训练

### 需要监控
1. **相机token差异**：
   - 初始化后：应该不同（已添加360/V度先验）
   - 提取时：可能被attention统一
   - 归一化后：可能被减弱
   - Shim后：应该被增强
   - 最终输出：应该不同

2. **训练过程中的变化**：
   - 损失应该驱动不同视角的参数分化
   - 如果仍然相同，可能需要更强的约束

## 训练策略建议

### 阶段1：启用view_proj训练（当前）
- view_proj可训练，trunk冻结
- 监控相机参数差异是否增加

### 阶段2：如果差异仍然小，解冻pose_branch
```python
# 在apply_freeze_train_strategy_precise中
if hasattr(camera_head, 'pose_branch'):
    for param in camera_head.pose_branch.parameters():
        param.requires_grad = True
```

### 阶段3：添加相机参数先验损失
```python
# 在损失函数中添加
def compute_camera_prior_loss(pose_enc, V):
    """
    利用360/V度均匀间隔的先验
    pose_enc: [B, V, 9] (rotation部分在前3个或前4个维度)
    """
    # 提取旋转部分（假设是四元数或旋转矩阵）
    # 计算期望的角度间隔
    expected_angles = torch.arange(V, device=pose_enc.device) * (360.0 / V)
    # 计算实际角度
    actual_angles = extract_rotation_angles(pose_enc)  # 需要实现
    # 计算损失
    angle_diff = (actual_angles - expected_angles.unsqueeze(0)) ** 2
    return angle_diff.mean()
```

## 调试步骤

1. **检查初始化**：
   ```python
   # 在训练开始前
   print("Camera token initial values:")
   for v in range(V):
       print(f"  View {v}: mean={camera_token[0, v, 0, :].mean():.6f}, std={camera_token[0, v, 0, :].std():.6f}")
       print(f"  First 5 dims: {camera_token[0, v, 0, :5].cpu().numpy()}")
   ```

2. **检查提取时的值**：
   - 在CameraHead.forward中添加打印
   - 查看pose_tokens_mv在不同视角的差异

3. **检查view_proj的效果**：
   - 在view_proj前后都打印差异
   - 确认view_proj是否增强了差异

4. **检查最终输出**：
   - 在loss函数中打印pose_enc的差异
   - 确认不同视角的参数确实不同

## 预期结果

训练几个epoch后，应该看到：
- 相机参数差异逐渐增加
- 不同视角的旋转角度接近360/V度的间隔
- L_cam_const损失应该增加（因为参数开始分化）

如果仍然相同，需要：
1. 检查梯度是否流到view_proj
2. 增加view_proj的学习率
3. 考虑解冻更多层

