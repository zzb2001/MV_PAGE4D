# mv_page4d_lite 网络结构完整分析（基于实际推理流程）

## 一、完整前向传播流程（基于 inference.py）

### 1.0 输入格式
- **输入**：`images_batch: [B, T, V, 3, H, W]`
  - `B=1`: batch size
  - `T=6`: 时间步数（可配置）
  - `V`: 视角数（例如4个视角）
  - `3`: RGB通道
  - `H=W=378`: 图像尺寸（可配置）

### 1.1 Stage-0: 第一次Forward（获取深度和相机参数）

**目的**：为体素化准备深度图和相机参数

**流程**：
```python
# 在 VGGT.forward() 中
if is_multi_view and self.enable_voxelization:
    with torch.set_grad_enabled(False):
        # 第一次aggregator调用（不使用体素化，因为depth=None）
        temp_tokens, temp_patch_idx, _, _ = self.aggregator(
            images=images,
            temporal_features=temporal_features,
            use_voxelization=True,  # 但实际不会体素化（因为depth=None）
        )
        
        # 获取相机参数
        pose_enc_temp = self.camera_head(temp_tokens)[-1]  # [B, V, 9]
        extrinsics_for_voxel, intrinsics_for_voxel = pose_encoding_to_extri_intri(...)
        
        # 获取深度
        depth_for_voxel, _ = self.depth_head(temp_tokens, images, temp_patch_idx)
```

**说明**：
- 这次forward不使用体素化（因为depth=None），走标准patch-based路径
- 输出：`depth_for_voxel: [B, T, V, H, W]`，`extrinsics: [B, T, V, 3, 4]`，`intrinsics: [B, T, V, 3, 3]`

### 1.2 Stage-0: Aggregator - Patch Embedding

**输入**：`images: [B, T, V, 3, H, W]`

**处理**：
```
[B, T, V, 3, H, W]
  ↓ reshape
[B*T*V, 3, H, W]  (S = T*V)
  ↓ ResNet归一化
[B*S, 3, H, W]
  ↓ DINOv2 ViT-L/14 PatchEmbed (patch_size=14)
[B*S, P, C]
```
- `P = (H//14) × (W//14) = 27 × 27 = 729` patches
- `C = embed_dim = 1024`

**输出**：`patch_tokens: [B*S, 729, 1024]`

### 1.3 Stage-0: ViewMixer（体素化路径）

**条件**：仅在体素化路径中使用（`use_voxel=True` 且 `depth/intrinsics/extrinsics` 不为None）

**流程**：
```python
# 在 aggregator.forward() 中
if is_multi_view and use_voxel and depth is not None and ...:
    patch_tokens_mv = patch_tokens.reshape(B, T, V, P, C)  # [B, T, V, 729, 1024]
    
    # Apply ViewMixer first
    patch_tokens_mv = self.viewmixer(patch_tokens_mv)  # [B, T, V, 729, 1024]
```

**ViewMixer内部**：
1. Reshape: `[B, T, V, 729, 1024]` → `[B*T, V*729, 1024]`
2. LoRA-based Cross-view Attention:
   - QKV投影使用LoRA (rank=16)
   - 多头注意力 (num_heads=8)
   - 注意力矩阵: `[B*T, V*729, V*729]`
3. Gated Residual: `gate * attention_out + (1-gate) * input`
4. LayerNorm
5. Reshape back: `[B*T, V*729, 1024]` → `[B, T, V, 729, 1024]`

**输出**：`patch_tokens_mv: [B, T, V, 729, 1024]`（跨视角对齐后的patch tokens）

### 1.4 Stage-0: Voxelization（体素化）

**条件**：`use_voxel=True` 且 `depth/intrinsics/extrinsics` 已提供

**流程**（每个时间步t独立处理）：
```python
for t in range(T):
    patch_tokens_t = patch_tokens_mv[:, t, :, :, :]  # [B, V, 729, 1024]
    depth_t = depth[:, t, :, :, :]  # [B, V, H, W]
    intrinsics_t = intrinsics[:, t, :, :, :]  # [B, V, 3, 3]
    extrinsics_t = extrinsics[:, t, :, :, :]  # [B, V, 3, 4]
    
    # VoxelizationModule
    voxel_tokens_batch, voxel_xyz_batch, voxel_ids_batch, voxel_mask_batch = \
        self.voxelization_module(
            patch_tokens_t, depth_t, intrinsics_t, extrinsics_t
        )
    # 返回: List of [N_t, C] per batch
```

**VoxelizationModule内部**：
1. 反投影像素到世界坐标：使用depth和相机参数
2. 计算体素索引：自适应体素大小（target_num_voxels=120000）
3. 体素内加权聚合：使用置信度softmax加权，scatter_add聚合
4. 生成体素tokens：特征投影 + 位置编码

**输出**：
- `voxel_tokens_list: T个 [B, N_t, C]`（每个时间步体素数可能不同）
- `voxel_xyz_list: T个 [B, N_t, 3]`
- `voxel_ids_list: T个 [B, N_t]`

**Padding**：将不同时间步的体素数统一到max_num_voxels
```python
# 找到最大体素数
max_num_voxels = max(...)
# Padding到统一长度
voxel_tokens_all: [B, T, max_num_voxels, C]
patch_tokens = voxel_tokens_all.reshape(B, T*max_num_voxels, C)  # [B, T*max_N, C]
P = max_num_voxels
S = T * max_num_voxels
```

### 1.5 Token组装与嵌入注入

#### Token类型

**1. Camera Token**
- 结构：`[1, V, 1, 1024]` → 展开为 `[B*T, 1, 1024]`（体素化路径）
- 跨时间共享：同一视角v在不同时间t使用相同的camera token

**2. Register Token**
- 结构：`[1, T, 4, 1024]` → 展开为 `[B*T, 4, 1024]`（体素化路径）
- 按时间组织：每个时间步t有4个register tokens

**3. Voxel Tokens**
- 来自体素化：`[B*T, max_num_voxels, 1024]`

#### Token拼接（体素化路径）

```
Camera:   [B*T, 1, 1024]
Register: [B*T, 4, 1024]
Voxel:    [B*T, max_num_voxels, 1024]
  ↓ concatenate
[B*T, 1+4+max_num_voxels, 1024] = [B*T, P, 1024]
```

#### 嵌入注入（体素化路径）

```python
# Time embedding: 每个时间步独立
time_ids = torch.arange(T).unsqueeze(0).expand(B, -1).reshape(B*T)  # [B*T]
time_emb = self.time_embed(time_ids).unsqueeze(1)  # [B*T, 1, 1024]

# View embedding: 体素化后不再需要（多视角已融合）
view_emb = torch.zeros(B*T, 1, 1024)

# Camera embedding: 不使用
cam_emb = torch.zeros(B*T, 1, 1024)

# 添加到tokens
camera_tokens_with_emb = tokens[:, 0:1, :] + cam_emb + view_emb
patch_tokens_with_emb = tokens[:, patch_start_idx:, :] + time_emb
tokens = torch.cat([camera_tokens_with_emb, tokens[:, 1:patch_start_idx, :], patch_tokens_with_emb], dim=1)
```

**注意**：体素化路径不使用RoPE（体素已有3D位置编码）

### 1.6 Transformer主干：24层交替注意力

#### 总体结构
- **24层**，每层包含：
  1. **Frame Attention**：帧内注意力
  2. **Global Attention**：全局序列注意力
- **输出**：每层concat frame和global → `[B*T, P, 2048]`

#### 三阶段路由策略（体素化路径）

##### Stage-1 (Layers 0-7): 同ID时间窗口路由

**Frame Attention (8层)**：
```python
# 体素化路径：使用体素ID进行跨时间步对齐
# 同一体素ID在不同时间步之间进行注意力
tokens: [B*T, P, 1024]
  ↓ reshape
[B, T, P, 1024]
  ↓ 使用voxel_ids进行分组
# 按体素ID分组，同一ID的体素在不同时间步间attention
tokens_out: [B, T, P, 1024]
  ↓ reshape back
[B*T, P, 1024]
```

**Global Attention (8层)**：
```python
tokens: [B*T, P, 1024]
  ↓ reshape
[B, T*P, 1024]
  ↓ Global Block
[B, T*P, 1024]
  ↓ reshape
[B*T, P, 1024]
```

**Layer 7后生成动态Mask**（如果启用）：
```python
cached_key_bias_1d, cached_cam_row_mask = spatial_mask_head(...)
# cache_mask: [B, S*P]  (动态区域mask)
```

##### Stage-2 (Layers 8-17): 同ID时间注意力路由

**Frame Attention (10层)**：
```python
# 体素化路径：时序路由
# 同一体素ID在不同时间步间进行时序注意力
tokens: [B*T, P, 1024]
  ↓ reshape
[B, T, P, 1024]
  ↓ 使用voxel_ids进行时序对齐
# 按体素ID分组，跨时间步时序attention
tokens_out: [B, T, P, 1024]
```

**Global Attention (10层)**：
```python
tokens: [B*T, P, 1024]
  ↓ apply mask (weak for camera, strong for register)
  ↓ Global Block with mask
[B*T, P, 1024]
```

**Mask应用**：
```python
gamma = torch.sigmoid(camera_mask_gate)  # 可学习门控
camera_mask = gamma * cache_mask  # 弱masking
register_mask = cache_mask  # 强masking
```

##### Stage-3 (Layers 18-23): 混合路由

**Frame Attention (6层)**：
- Layers 18-20: 同ID时间注意力（如Stage-2）
- Layers 21-23: 同ID时间窗口（如Stage-1）

**Global Attention (6层)**：
- 继续应用mask和门控机制

#### 每层输出聚合

```python
# Frame intermediate: [B*T, P, 1024]
# Global intermediate: [B*T, P, 1024]
concat_inter = torch.cat([frame_intermediates[i], global_intermediates[i]], dim=-1)
# Output: [B*T, P, 2048]
```

**24层输出列表**：`aggregated_tokens_list[24]`，每层 `[B*T, P, 2048]`

### 1.7 输出头 (Heads)

#### CameraHead

**输入**：`aggregated_tokens_list[-1]` → `[B*T, P, 2048]`

**处理流程**（多视角模式）：
```python
# 提取Camera Tokens
tokens_mv = tokens.reshape(B, T, P, C)
# 从第一个时间步提取每个视角的camera token
pose_tokens_mv = tokens_mv[:, 0, :, :]  # 简化：实际需要从特殊token位置提取
pose_tokens_mv = tokens_mv[:, 0, 0, :]  # [B, V, C]（假设camera token在位置0）

# View-Specific Projection
for v_idx in range(V):
    pose_tokens_v = self.view_proj[v_idx](pose_tokens_mv[:, v_idx:v_idx+1, :])

# 迭代细化（每个视角独立，4次迭代）
for v_idx in range(V):
    pred_pose_enc_list_v = self.trunk_fn(pose_tokens_v, num_iterations=4)
```

**输出**：`pose_enc: [B, V, 9]`
- 平移 (3维)
- 四元数旋转 (4维)
- FOV (2维)

#### DepthHead (DPTHead)

**输入**：
- `aggregated_tokens_list` (24层输出)
- `images: [B, T, V, 3, H, W]`

**处理流程**：
1. 提取中间层特征：layers [4, 11, 17, 23]
2. Projection: `[B*T, P, 2048]` → `[256, 512, 1024, 1024]`
3. Resize layers: 上采样/下采样
4. Fusion blocks: 4层特征融合
5. Output conv: 生成深度和置信度

**输出**：
- `depth: [B, T, V, 1, H, W]` 或 `[B, T, V, H, W]`
- `depth_conf: [B, T, V, 1, H, W]` (值域 [1, +∞))

#### PointHead (DPTHead)

**结构类似DepthHead**

**输出**：
- `world_points: [B, T, V, 3, H, W]` (3D世界坐标)
- `world_points_conf: [B, T, V, 1, H, W]`

#### VoxelGaussianHead（Teacher端，体素化模式）

**输入**：
- `voxel_tokens: [B, T, N_voxels, 2048]`
- `voxel_xyz: [B, T, N_voxels, 3]`
- `voxel_size: float`

**处理流程**：
1. 从体素tokens直接预测高斯参数
2. 输出83维高斯参数

**输出**：
- `teacher_gaussian_params: [B, T, N_voxels, 83]`

#### FusionHead + FusedGaussianHead（Student端）

**输入**：
- `voxel_tokens: [B, T, V, N_voxels, 2048]` 或 `[B, T, N_voxels, 2048]`
- `voxel_xyz: [B, T, N_voxels, 3]`
- `voxel_ids: [B, T, N_voxels]`
- `static_mask: [B, T, H, W]` (可选)

**FusionHead处理**（每个时间步t）：
1. 可见性过滤：只保留在多视角中可见的体素
2. 置信度加权聚合：使用置信度进行softmax加权平均
3. 坐标去重：基于体素ID去重
4. 特征融合：MLP(C → C//4)
5. 分离静态/动态高斯（如果提供static_mask）

**输出**：
- `fused_features: [B, T, N_t, 256]`
- `voxel_xyz_fused: [B, T, N_t, 3]`
- `G_t_full: [B, T, N_t, 83]`
- `G_t_static: List` (静态高斯列表)
- `G_t_dynamic: List` (动态高斯列表)

**FusedGaussianHead处理**：
- 输入：`fused_features: [B, T, N_t, 256]`
- 输出：`fused_gaussian_params: [B, T, N_t, 83]`

#### GaussianRenderer（可微渲染）

**输入**：
- `fused_gaussian_params: [B, T, N_t, 83]`
- `fused_gaussian_xyz: [B, T, N_t, 3]`
- `intrinsics: [B, T, V, 3, 3]`
- `extrinsics: [B, T, V, 3, 4]`
- `image_size: (H, W)`

**处理**：
- 使用gsplat库进行可微高斯渲染
- 将融合后的高斯参数渲染回各视角

**输出**：
- `rendered_images: [B, T, V, 3, H, W]`
- `rendered_depth: [B, T, V, H, W]`
- `rendered_alpha: [B, T, V, H, W]`
- `gaussian_visibility: [B, T, V, N]`

---

## 二、完整前向传播维度变化表（体素化路径）

### 2.1 维度变化完整流程

| 阶段 | 模块 | 输入维度 | 输出维度 | 说明 |
|------|------|---------|---------|------|
| **输入** | Images | `[B, T, V, 3, H, W]` | - | B=1, T=6, V=4, H=W=378 |
| **第一次Forward** | Aggregator | `[B, T, V, 3, H, W]` | `temp_tokens: [B*T*V, 734, 1024]` | 获取深度和相机参数（不使用体素化） |
| **第一次Forward** | CameraHead | `temp_tokens` | `pose_enc: [B, V, 9]` | 4次迭代细化 |
| **第一次Forward** | DepthHead | `temp_tokens` | `depth: [B, T, V, H, W]` | DPT架构 |
| **主Forward** | Patch Embed | `[B, T, V, 3, H, W]` | `[B*T*V, 729, 1024]` | DINOv2 ViT-L/14 |
| **主Forward** | ViewMixer | `[B, T, V, 729, 1024]` | `[B, T, V, 729, 1024]` | 跨视角对齐（仅体素化路径） |
| **主Forward** | Voxelization | `[B, V, 729, 1024]` × T | `List[B, N_t, 1024]` × T | 每个时间步独立体素化 |
| **主Forward** | Padding | `List[B, N_t, 1024]` × T | `[B, T, max_N, 1024]` | 统一体素数 |
| **主Forward** | Token组装 | - | `[B*T, 1+4+max_N, 1024]` | Camera+Register+Voxel |
| **主Forward** | 嵌入注入 | `[B*T, P, 1024]` | `[B*T, P, 1024]` | Time embedding |
| **Stage-1** | Frame (0-7) | `[B*T, P, 1024]` | `[B*T, P, 1024]` | 同ID时间窗口路由 |
| **Stage-1** | Global (0-7) | `[B, T*P, 1024]` | `[B, T*P, 1024]` | 全局注意力 |
| **Mask生成** | SpatialMaskHead | `[B, T, P, 1024]` | `cache_mask: [B, T*P]` | Layer 7后（可选） |
| **Stage-2** | Frame (8-17) | `[B*T, P, 1024]` | `[B*T, P, 1024]` | 同ID时间注意力路由 |
| **Stage-2** | Global (8-17) | `[B, T*P, 1024]` | `[B, T*P, 1024]` | 带mask的全局注意力 |
| **Stage-3** | Frame (18-23) | `[B*T, P, 1024]` | `[B*T, P, 1024]` | 混合路由 |
| **Stage-3** | Global (18-23) | `[B, T*P, 1024]` | `[B, T*P, 1024]` | 带mask的全局注意力 |
| **特征聚合** | Concat | Frame/Global各`[B*T, P, 1024]` | `[B*T, P, 2048]` | 每层输出 |
| **CameraHead** | Iterative Refine | `[B*T, P, 2048]` | `[B, V, 9]` | 4次迭代 |
| **DepthHead** | DPT | `[B*T, P, 2048]` × 4层 | `[B, T, V, H, W]` | 深度图 |
| **PointHead** | DPT | `[B*T, P, 2048]` × 4层 | `[B, T, V, 3, H, W]` | 3D点云 |
| **VoxelGaussianHead** | MLP | `[B, T, N_voxels, 2048]` | `[B, T, N_voxels, 83]` | Teacher端高斯参数 |
| **FusionHead** | Multi-view | `[B, T, V, N_voxels, 2048]` | `[B, T, N_t, 256]` | V→1融合 |
| **FusedGaussianHead** | MLP | `[B, T, N_t, 256]` | `[B, T, N_t, 83]` | Student端高斯参数 |
| **GaussianRenderer** | gsplat | `[B, T, N_t, 83]` | `[B, T, V, 3, H, W]` | 渲染回各视角 |

---

## 三、关键设计要点

### 3.1 两阶段Forward策略

**第一次Forward（获取深度和相机参数）**：
- 目的：为体素化准备深度图和相机参数
- 不使用体素化（因为depth=None），走标准patch-based路径
- 输出：`depth_for_voxel: [B, T, V, H, W]`，`extrinsics: [B, T, V, 3, 4]`，`intrinsics: [B, T, V, 3, 3]`

**主Forward（使用体素化）**：
- 使用第一次forward获取的深度和相机参数进行体素化
- 完整流程：Patch Embedding → ViewMixer → Voxelization → Transformer → 输出头

### 3.2 体素化路径的特殊处理

**ViewMixer的使用**：
- 仅在体素化路径中使用（`use_voxel=True` 且 `depth/intrinsics/extrinsics` 不为None）
- 在体素化之前进行跨视角对齐，提高体素化质量

**体素化后的路由策略**：
- Stage-1：同ID时间窗口路由（使用体素ID进行跨时间步对齐）
- Stage-2：同ID时间注意力路由（时序建模）
- Stage-3：混合路由（平衡时序和空间信息）

**嵌入策略**：
- 体素化路径不使用RoPE（体素已有3D位置编码）
- 只使用Time embedding，不使用View embedding（多视角已融合）

### 3.3 输出头处理

**Teacher端**：
- VoxelGaussianHead：每个视角独立预测高斯参数

**Student端**：
- FusionHead：多视角融合为统一表示
- FusedGaussianHead：从融合特征预测统一高斯参数
- GaussianRenderer：渲染回各视角

---

## 四、相对于 vggt_t_mask_mlp_fin10 的主要修改

- **示例**：`[1, 2, 4, 3, 378, 378]` = 1个batch，2个时间步，4个视角

#### Legacy模式（向后兼容）
- **输入格式**：`[B, S, 3, H, W]`
  - `S`: 序列长度 (单视角时序)

### 1.2 图像归一化与Patch Embedding

#### 维度变化流程

```
输入: [B, T, V, 3, H, W]
  ↓ reshape
[B*T*V, 3, H, W] = [B*S, 3, H, W]  (S = T*V)
  ↓ 归一化 (ResNet mean/std)
[B*S, 3, H, W]
  ↓ DINOv2 ViT-L/14 PatchEmbed (patch_size=14)
[B*S, P, C]
```

**详细维度**：
- `P = (H//patch_size) × (W//patch_size)`
  - 对于 378×378: `P = 27 × 27 = 729` patches
- `C = embed_dim = 1024`

**输出**：`[B*S, 729, 1024]`

### 1.3 ViewMixer (Stage-0) - **核心新增模块**

#### 功能
跨视角特征对齐，在Transformer主网络之前进行轻量级多视角融合。

#### 维度变化

```
输入: [B*S, 729, 1024]  (S = T*V)
  ↓ reshape to multi-view format
[B, T, V, 729, 1024]
  ↓ ViewMixer (cross-view attention within each time step)
[B, T, V, 729, 1024]
  ↓ reshape back
[B*S, 729, 1024]
```

#### ViewMixer内部处理
1. **Reshape**: `[B, T, V, 729, 1024]` → `[B*T, V, 729, 1024]` → `[B*T, V*729, 1024]`
2. **LoRA-based Attention**（默认模式）:
   - **QKV投影**：使用LoRA (rank=16)，参数量小
     - `q_lora = LoRALinear(1024, 1024, rank=16)`
     - `k_lora = LoRALinear(1024, 1024, rank=16)`
     - `v_lora = LoRALinear(1024, 1024, rank=16)`
   - **多头注意力**：num_heads=8, head_dim=128
   - **注意力计算**：`[B*T, V*729, V*729]` 注意力矩阵
   - **输出投影**：`proj_lora = LoRALinear(1024, 1024, rank=16)`
3. **Gated Residual**：`gate = sigmoid(gate_param)`, `output = gate * attention_out + (1-gate) * input`
4. **LayerNorm**：最终归一化
5. **输出**: `[B*T, V*729, 1024]` → reshape回 `[B, T, V, 729, 1024]` → `[B*S, 729, 1024]`

**替代模式**（非LoRA）：
- 使用窄宽度投影（narrow_dim = embed_dim // 4 = 256）
- 初始化接近单位矩阵，保持轻量级

**关键点**：每个时间步t内，不同视角v之间进行注意力，实现跨视角对齐。参数量极小（LoRA rank=16），几乎零侵入。

### 1.4 Token组装与特殊Token管理

#### Token类型

**1. Camera Token**
- **结构**：`[1, V, 1, 1024]`
  - 每个视角一个camera token
  - **跨时间共享**：同一视角v在不同时间t使用相同的camera token
  - **语义**：表示相机参数（外参、内参），应在时间上保持一致
  - **初始化策略**：使用视角先验知识（均匀角度分布）进行初始化，确保不同视角token有差异

- **展开过程**：
  ```
  [1, V, 1, 1024]
    ↓ expand to batch
  [B, V, 1, 1024]
    ↓ expand along time dimension
  [B, T, V, 1, 1024]
    ↓ reshape to flat
  [B*T*V, 1, 1024] = [B*S, 1, 1024]
  ```

**2. Register Token**
- **结构**：`[1, T, 4, 1024]`
  - 每个时间步t有4个register tokens
  - **按时间组织**：不同时间步有不同的register tokens
  - **语义**：表示时间相关的全局信息

- **展开过程**：
  ```
  [1, T, 4, 1024]
    ↓ expand to batch
  [B, T, 4, 1024]
    ↓ expand along view dimension
  [B, T, V, 4, 1024]
    ↓ reshape to flat
  [B*T*V, 4, 1024] = [B*S, 4, 1024]
  ```

**3. Patch Tokens / Voxel Tokens**
- **非体素化路径**：来自ViewMixer的patch tokens `[B*S, 729, 1024]`
- **体素化路径**：来自VoxelizationModule的voxel tokens `[B*T, N_voxels, 1024]`
  - 体素化后，每个时间步t的V个视角融合为统一的体素表示
  - 体素数N_voxels可能随时间步变化（自适应体素大小）

### 1.5 位置嵌入与元信息嵌入

#### Rotary Position Embedding (RoPE)
- **应用对象**：Patch tokens only
- **特殊tokens位置**：设为0（camera和register tokens不使用空间位置）

#### 新增嵌入（相比vggt_t_mask_mlp_fin10）

**1. Time Embedding**
```python
time_embed = nn.Embedding(max_time_steps=100, embed_dim=1024)
```
- **注入**：添加到patch tokens
- **维度**：`[B*S, 1, 1024]` → broadcast到 `[B*S, 729, 1024]`
- **作用**：显式编码时间信息

**2. View Embedding**
```python
view_embed = nn.Embedding(max_views=32, embed_dim=1024)
```
- **注入**：添加到patch tokens
- **维度**：`[B*S, 1, 1024]` → broadcast到 `[B*S, 729, 1024]`
- **作用**：显式编码视角信息

**3. Camera Parameter Embedding**
```python
camera_param_embed = nn.Sequential(
    nn.Linear(1, 256),
    nn.GELU(),
    nn.Linear(256, 1024)
)
```
- **注入**：添加到camera token
- **输入**：view_id (归一化)
- **维度**：`[B*S, 1, 1024]`
- **作用**：编码相机参数信息

**最终tokens**：`[B*S, 734, 1024]`（已包含所有嵌入）

### 1.6 Transformer主干：24层交替注意力

#### 总体结构
- **24层**，每层包含：
  1. **Frame Attention**：帧内注意力
  2. **Global Attention**：全局序列注意力
- **输出**：每层concat frame和global → `[B, S, 734, 2048]`

#### 三阶段路由策略（**核心创新**）

##### Stage-1 (Layers 0-7): 多视角聚合阶段

**Frame Attention (8层)**：
```
输入: [B*S, 734, 1024]  (S = T*V)
  ↓ reshape
[B, T, V, 734, 1024]
  ↓ multi-view routing: reshape for same-time cross-view attention
[B*T, V*734, 1024]
  ↓ Frame Block (Transformer)
[B*T, V*734, 1024]
  ↓ reshape back
[B*S, 734, 1024]
```

**目的**：同一时间步t内，不同视角v之间进行注意力，聚合多视角几何信息。

**Global Attention (8层)**：
```
输入: [B, S*734, 1024]
  ↓ Global Block (Transformer)
[B, S*734, 1024]
  ↓ reshape
[B, S, 734, 1024]
```

**Layer 7后生成动态Mask**：
```python
cached_key_bias_1d, cached_cam_row_mask = spatial_mask_head(...)
# cached_value: [B, S*P]
# cache_mask: [B, S*P]  (动态区域mask)
```

##### Stage-2 (Layers 8-17): 时序建模阶段

**Frame Attention (10层)**：
```
输入: [B*S, 734, 1024]
  ↓ reshape
[B, T, V, 734, 1024]
  ↓ temporal routing: reshape for same-view cross-time attention
[B*V, T*734, 1024]
  ↓ Frame Block (Transformer)
[B*V, T*734, 1024]
  ↓ reshape back
[B*S, 734, 1024]
```

**目的**：同一视角v内，不同时间步t之间进行注意力，建模时序一致性。

**Global Attention (10层)**：
```
输入: [B, S*734, 1024]
  ↓ apply mask (weak for camera, strong for register)
  ↓ Global Block with mask
[B, S*734, 1024]
```

**Mask应用**：
```python
gamma = torch.sigmoid(camera_mask_gate)  # 可学习门控
camera_mask = gamma * cache_mask  # 弱masking
register_mask = cache_mask  # 强masking
```

##### Stage-3 (Layers 18-23): 混合路由阶段

**Frame Attention (6层)**：
- Layers 18-20: Temporal routing（如Stage-2）
- Layers 21-23: Multi-view routing（如Stage-1）

**目的**：平衡时序和视角信息，进行最终融合。

**Global Attention (6层)**：
- 继续应用mask和门控机制

#### 每层输出聚合

```python
# Frame intermediate: [B, S, 734, 1024]
# Global intermediate: [B, S, 734, 1024]
concat_inter = torch.cat([frame_intermediates[i], global_intermediates[i]], dim=-1)
# Output: [B, S, 734, 2048]
```

**24层输出列表**：`output_list[24]`，每层 `[B, S, 734, 2048]`

### 1.7 输出头 (Heads)

#### CameraHead

**输入**：`aggregated_tokens_list[-1]` → `[B, T*V, 734, 2048]` 或 `[B*T, P, 2048]`（体素化路径）

**处理流程**（多视角模式）：
1. **提取Camera Tokens**：
   ```
   [B, T*V, 734, 2048] 或 [B*T, P, 2048]
     ↓ reshape to [B, T, V, 734, 2048] 或推断形状
     ↓ extract camera tokens from first time step
   [B, V, 2048]  (每个视角一个camera token)
   ```

2. **View-Specific Projection**（新增）：
   - 每个视角v使用独立的投影层 `view_proj[v]`
   - 结构：`Linear(2048 → 512) → GELU → Linear(512 → 2048)`
   - 初始化：小权重（std=0.01），保持视角差异
   - 目的：在冻结trunk之前学习视角特定的变换

3. **迭代细化** (4次迭代，每个视角独立)：
   - **Trunk**：4层Transformer blocks（冻结）
   - **Adaptive LayerNorm调制**：使用pose encoding调制
   - **Pose branch**：MLP输出9维参数
   - 对每个视角v分别调用 `trunk_fn(pose_tokens_v, num_iterations=4)`

**输出**：`pose_enc: [B, V, 9]`
- 平移 (3维)
- 四元数旋转 (4维)
- FOV (2维)

**关键点**：
- 相机参数跨时间共享，从第一个时间步提取
- View-specific projection确保不同视角的相机参数有差异
- 每个视角独立迭代细化，避免视角间混淆

#### DepthHead (DPTHead)

**输入**：
- `aggregated_tokens_list` (24层输出)
- `images: [B, T, V, 3, H, W]`

**处理流程**：
1. **提取中间层特征**：layers [4, 11, 17, 23]
2. **Projection**: `[B, S, P, 2048]` → `[256, 512, 1024, 1024]`
3. **Resize layers**: 上采样/下采样
4. **Fusion blocks**: 4层特征融合
5. **Output conv**: 生成深度和置信度

**输出**：
- `depth: [B, T, V, 1, H, W]`
- `depth_conf: [B, T, V, 1, H, W]` (值域 [1, +∞))

#### PointHead (DPTHead)

**结构类似DepthHead**

**输出**：
- `world_points: [B, T, V, 3, H, W]` (3D世界坐标)
- `world_points_conf: [B, T, V, 1, H, W]`

#### TrackHead (可选)

**输入**：
- `aggregated_tokens_list`
- `query_points: [N, 2]` (像素坐标)

**输出**：
- `tracks: [B, T, N, 2]` (N个查询点的轨迹)
- `vis: [B, T, N]` (可见性)
- `conf: [B, T, N]` (置信度)

#### VoxelGaussianHead (体素化模式)

**输入**：
- `voxel_tokens: [B, T, N_voxels, 2048]` 或 `[B, T, V, N_voxels, 2048]`
- `voxel_xyz: [B, T, N_voxels, 3]`
- `voxel_size: float`

**处理流程**：
1. 从体素tokens直接预测高斯参数
2. 输出83维高斯参数（opacity, scales, rotations, SH coefficients）

**输出**：
- `gaussian_params: [B, T, N_voxels, 83]`

#### VGGT_DPT_GS_Head (像素级模式)

**输入**：
- `aggregated_tokens_list`（24层输出）
- `images: [B, T, V, 3, H, W]`

**处理流程**：
1. 提取中间层特征：layers [4, 11, 17, 23]
2. DPT架构处理多尺度特征
3. 直接图像特征融合（input_merger）
4. 输出高斯参数

**输出**：
- `gaussian_params: [B, T, V, 83, H, W]`

---

## 五、体素化与融合模块（新增）

### 5.1 VoxelizationModule (Stage-0 体素化)

#### 功能
将多视角像素/patch特征体素化到统一的世界网格，生成稀疏的体素tokens。

#### 核心流程

```
输入：
- patch_tokens: [B, V, P, C]  (每个时间步t)
- depth: [B, V, H, W]
- intrinsics: [B, V, 3, 3]
- extrinsics: [B, V, 3, 4]
- confidence: [B, V, H, W] (可选)
- pixel_mask: [B, V, H, W] (可选)

处理：
1. 反投影像素到世界坐标
   - 使用depth和相机参数计算3D点
   - points3d: [B*V*N, 3]

2. 计算体素索引
   - 自适应体素大小：根据target_num_voxels=120000自动计算
   - 或固定体素大小
   - 体素索引: (ix, iy, iz) → voxel_id

3. 体素内加权聚合（AnySplat范式）
   - 使用置信度softmax加权
   - scatter_add聚合特征
   - voxel_feat: [N_voxels, C]

4. 生成体素tokens
   - 特征投影：MLP(patch_feat) → [N_voxels, C]
   - 位置编码：MLP(voxel_xyz) → [N_voxels, pos_dim]
   - 融合：voxel_token = feat_proj + pos_encoding

输出：
- voxel_tokens: List of [N_t, C] (每个batch和时间步的体素数可能不同)
- voxel_xyz: List of [N_t, 3]
- voxel_ids: List of [N_t]
- voxel_mask: List of [N_t] (如果提供pixel_mask)
```

#### 关键特性

1. **自适应体素大小**：
   - 根据场景范围自动计算体素大小
   - 目标体素数：120000
   - 公式：`voxel_size = scene_extent / (target_num_voxels^(1/3))`

2. **体素ID生成**：
   - 支持Morton编码（Z-order）或简单索引
   - 用于跨时间步对齐相同体素

3. **边界稳定性**：
   - 边界容忍度处理，避免边界体素不稳定

4. **置信度加权**：
   - 使用深度置信度进行softmax加权聚合
   - 提高高质量区域的特征权重

### 5.2 FusionHead (多视角融合)

#### 功能
将V个视角的体素特征融合为时刻t的统一表示（V→1融合）。

#### 核心流程

```
输入：
- voxel_tokens: [B, T, V, N_t, C] 或 [B, T, N_t, C]
- voxel_xyz: [B, T, V, N_t, 3] 或 [B, T, N_t, 3]
- voxel_ids: [B, T, N_t] (可选)
- confidence: [B, T, V, N_t] (可选)
- visibility: [B, T, V, N_t] (可选)
- static_mask: [B, T, N_t] (可选)

处理（每个时间步t）：
1. 可见性过滤
   - 只保留在至少一个视角中可见的体素
   - visible_mask: [B, N_t]

2. 置信度加权聚合
   - 归一化权重：softmax(confidence, dim=1)
   - 加权平均：fused_tokens = Σ(conf_weight * tokens)

3. 坐标去重（基于体素ID）
   - 相同ID的体素坐标取平均
   - 使用torch.unique和index_add

4. 特征融合
   - feature_fusion: MLP(C → C//4)
   - 输出：fused_features: [B, T, N_t, C//4]

5. 分离静态/动态高斯
   - 使用static_mask分离静态和动态体素
   - G_t_static: List of [N_static, 83]
   - G_t_dynamic: List of [N_dynamic, 83]

输出：
- P_t_full: [B, T, N_t, 3]  (完整点云)
- G_t_full: [B, T, N_t, 83] (完整高斯参数)
- G_t_static: List (静态高斯列表)
- G_t_dynamic: List (动态高斯列表)
- voxel_xyz_fused: [B, T, N_t, 3]
- voxel_ids_fused: [B, T, N_t]
- fused_features: [B, T, N_t, C//4]
```

#### 关键特性

1. **可见性阈值**：默认0.5，只保留多视角可见的体素
2. **置信度阈值**：默认0.1，过滤低置信度体素
3. **静态/动态分离**：支持基于mask的静态/动态分离
4. **法线/颜色一致性**（可选）：用于去重和一致性检查

### 5.3 FusedGaussianHead (融合高斯头)

#### 功能
从融合后的体素特征预测统一的高斯参数（Student端）。

#### 输入
- `fused_features: [B, T, N_t, embed_dim//4]`
- `voxel_xyz_fused: [B, T, N_t, 3]`
- `voxel_size: float`

#### 输出
- `gaussian_params: [B, T, N_t, 83]`
- `gaussian_xyz: [B, T, N_t, 3]`
- `opacity, scales, rotations, sh_coeffs`

### 5.4 GaussianRenderer (高斯渲染器)

#### 功能
使用gsplat库进行可微高斯渲染，将融合后的高斯参数渲染回各视角。

#### 输入
- `gaussian_params: [B, T, N_t, 83]`
- `gaussian_xyz: [B, T, N_t, 3]`
- `intrinsics: [B, T, V, 3, 3]`
- `extrinsics: [B, T, V, 3, 4]`
- `image_size: (H, W)`

#### 输出
- `rendered_images: [B, T, V, 3, H, W]`
- `rendered_depth: [B, T, V, H, W]`
- `rendered_alpha: [B, T, V, H, W]`
- `visibility: [B, T, V, N]` (高斯可见性)

#### 渲染参数
- `background_color: (0.0, 0.0, 0.0)` (黑色背景)
- `near_plane: 1e-10`
- `far_plane: None`
- `radius_clip: 0.1`
- `rasterize_mode: 'classic'`

### 5.5 TemporalTracker (时序追踪器)

#### 功能
跨时间步追踪体素，建立时序关联。

#### 实现
- 使用体素ID进行跨时间步对齐
- 支持时序窗口（temporal_window=3）

---

## 二、相对于 vggt_t_mask_mlp_fin10 的主要修改

### 2.1 核心架构差异对比

| 组件 | vggt_t_mask_mlp_fin10 (单视角时序) | mv_page4d_lite (多视角时序) |
|------|-----------------------------------|----------------------------|
| **输入格式** | `[B, S, 3, H, W]` | `[B, T, V, 3, H, W]` + Legacy兼容 |
| **Token结构** | `[1, 2, 1, C]` (第一帧 vs 其他帧) | `[1, V, 1, C]` (每视角) + `[1, T, 4, C]` (每时间步) |
| **Stage-0** | ❌ 无 | ✅ ViewMixer (跨视角对齐) |
| **Frame Attention路由** | 标准序列注意力 | 三阶段路由策略 |
| **位置嵌入** | RoPE only | RoPE + Time/View/Camera嵌入 |
| **Mask门控** | 固定mask强度 | 可学习门控 γ |

### 4.2 详细修改分析

#### ✅ 修改1: 新增 ViewMixer + Voxelization (Stage-0，体素化路径)

**vggt_t_mask_mlp_fin10**：
- 直接使用patch embedding的输出进入Transformer

**mv_page4d_lite（体素化路径）**：
```python
# 1. Patch Embedding
patch_tokens = patch_embed(images)  # [B*T*V, P, C]

# 2. ViewMixer（仅在体素化路径中使用）
if use_voxel and depth is not None:
    patch_tokens_mv = patch_tokens.reshape(B, T, V, P, C)
    patch_tokens_mv = self.viewmixer(patch_tokens_mv)  # 跨视角attention
    # 然后进行体素化
    for t in range(T):
        voxel_tokens = self.voxelization_module(patch_tokens_mv[:, t], ...)
```

**设计理念**：
- **早期融合**：在体素化之前进行跨视角对齐，提高体素化质量
- **轻量级**：使用LoRA降低参数量（rank=16）
- **门控残差**：保留原始特征，允许渐进式融合
- **体素化**：将多视角特征统一到世界坐标系

**合理性**：✅ **非常合理**
- ViewMixer在体素化之前对齐视角，提高体素化质量
- 体素化将多视角特征统一到世界网格，便于后续处理

#### ✅ 修改2: 多视角Token结构重组

**vggt_t_mask_mlp_fin10**：
```python
# Camera token: [1, 2, 1, C]
# - 第一帧使用 index 0
# - 其他帧使用 index 1
camera_token = slice_expand_and_flatten(self.camera_token, B, S)
# 结果: [B, S, 1, C] → [B*S, 1, C]
```

**mv_page4d_lite**：
```python
# Camera token: [1, V, 1, C]
# - 每个视角一个token
# - 跨时间共享
camera_token_mv = self.camera_token[:, :V, :, :].expand(B, -1, -1, -1)  # [B, V, 1, C]
camera_token = camera_token_mv.unsqueeze(1).expand(-1, T, -1, -1, -1)  # [B, T, V, 1, C]
camera_token = camera_token.reshape(B*T*V, 1, C)  # [B*S, 1, C]

# Register token: [1, T, 4, C]
# - 每个时间步4个tokens
register_token_mv = self.register_token[:, :T, :, :].expand(B, -1, -1, -1)  # [B, T, 4, C]
register_token = register_token_mv.unsqueeze(2).expand(-1, -1, V, -1, -1)  # [B, T, V, 4, C]
register_token = register_token.reshape(B*T*V, 4, C)  # [B*S, 4, C]
```

**语义对比**：
- **vggt_t_mask_mlp_fin10**: 区分"第一帧"和"其他帧"，适用于单视角时序
- **mv_page4d_lite**: 区分"视角"和"时间步"，符合多视角时序语义

**合理性**：✅ **合理且必要**
- 相机参数应在时间上保持一致（同一视角的相机外参不应随时间变化）
- Register tokens按时间组织，可以更好地建模时序全局信息

#### ✅ 修改3: 体素化路径的三阶段路由策略（**最重要创新**）

**vggt_t_mask_mlp_fin10**：
- 所有frame attention都是标准序列注意力：`[B*S, P, C]` → Transformer → `[B*S, P, C]`
- 没有区分多视角和时序路由

**mv_page4d_lite（体素化路径）**：

**Stage-1 (0-7层): 同ID时间窗口路由**
```python
# 体素化路径：使用体素ID进行跨时间步对齐
# 同一体素ID在不同时间步之间进行注意力
tokens: [B*T, P, 1024]
  ↓ reshape
[B, T, P, 1024]
  ↓ 使用voxel_ids进行分组
# 按体素ID分组，同一ID的体素在不同时间步间attention
tokens_out: [B, T, P, 1024]
```
**目的**：利用体素ID对齐，建立跨时间步的空间对应

**Stage-2 (8-17层): 同ID时间注意力路由**
```python
# 体素化路径：时序路由
# 同一体素ID在不同时间步间进行时序注意力
tokens: [B*T, P, 1024]
  ↓ reshape
[B, T, P, 1024]
  ↓ 使用voxel_ids进行时序对齐
# 按体素ID分组，跨时间步时序attention
tokens_out: [B, T, P, 1024]
```
**目的**：建模时序一致性，跟踪动态物体

**Stage-3 (18-23层): 混合路由**
```python
# 前3层: 同ID时间注意力（如Stage-2）
# 后3层: 同ID时间窗口（如Stage-1）
use_temporal = (num_block - 18) < 3
stage_strategy = "stage2" if use_temporal else "stage1"
```
**目的**：平衡时序和空间信息，进行最终融合

**合理性**：✅ **非常合理，是核心创新**
- 体素化路径利用体素ID进行跨时间步对齐，比简单的reshape更有效
- 符合多视角时序重建的认知过程：
  1. 先建立空间对应（Stage-1）
  2. 再跟踪时序变化（Stage-2）
  3. 最后融合平衡（Stage-3）

#### ✅ 修改4: 体素化路径的嵌入策略

**vggt_t_mask_mlp_fin10**：
- 仅使用RoPE作为位置嵌入

**mv_page4d_lite（体素化路径）**：
```python
# Time embedding（仅体素化路径）
time_ids = torch.arange(T).unsqueeze(0).expand(B, -1).reshape(B*T)  # [B*T]
time_emb = self.time_embed(time_ids).unsqueeze(1)  # [B*T, 1, C]
# 添加到voxel tokens

# View embedding: 不使用（多视角已融合）
view_emb = torch.zeros(B*T, 1, C)

# Camera embedding: 不使用
cam_emb = torch.zeros(B*T, 1, C)

# RoPE: 不使用（体素已有3D位置编码）
pos = None
```

**合理性**：✅ **合理**
- 体素化后多视角已融合，不需要View embedding
- 体素已有3D位置编码，不需要RoPE
- Time embedding仍然有用，用于区分不同时间步

#### ✅ 修改5: 可学习的相机Mask门控

**vggt_t_mask_mlp_fin10**：
- Mask强度固定：`cache_mask` 直接应用

**mv_page4d_lite**：
```python
# 可学习门控
self.camera_mask_gate = nn.Parameter(torch.zeros(1))

# 应用
gamma = torch.sigmoid(self.camera_mask_gate)
camera_mask = gamma * cache_mask  # 弱masking for camera branch
register_mask = cache_mask  # 强masking for register branch
```

**合理性**：✅ **合理，但有改进空间**
- 允许模型自适应调整mask强度
- **改进建议**：可以尝试per-layer门控而非单一标量

#### ✅ 修改6: 输入格式自动检测

**vggt_t_mask_mlp_fin10**：
- 仅支持 `[B, S, 3, H, W]`

**mv_page4d_lite**：
```python
# 自动检测
is_multi_view = len(images.shape) == 6
if is_multi_view:
    B, T, V, C, H, W = images.shape
    S = T * V
else:
    B, S, C, H, W = images.shape
    T = S
    V = 1
```

**合理性**：✅ **必要且实现良好**
- 保持向后兼容性
- 清晰的格式检测和统一处理

#### ✅ 修改7: CameraHead多视角适配

**vggt_t_mask_mlp_fin10**：
- 提取所有帧的camera token: `tokens[:, :, 0]` → `[B, S, C]`
- 输出: `[B, S, 9]`

**mv_page4d_lite**：
```python
# 从多视角tokens中提取
tokens_mv = tokens.reshape(B, T, V, P, C)
pose_tokens_mv = tokens_mv[:, 0, :, 0, :]  # [B, V, C]  (从第一个时间步提取)
# 分别处理每个视角
for v_idx in range(V):
    pred_pose_enc_list_v = self.trunk_fn(pose_tokens_v, num_iterations)
# 输出: [B, V, 9]
```

**合理性**：✅ **合理**
- 符合相机参数跨时间共享的要求
- 从第一个时间步提取是合理的简化

---

## 五、完整前向传播流程总结

### 5.1 推理流程（基于 inference.py）

```
输入: images_batch [B, T, V, 3, H, W]
  ↓
第一次Forward（获取深度和相机参数）
  - Aggregator（不使用体素化，因为depth=None）
  - CameraHead → pose_enc [B, V, 9]
  - DepthHead → depth [B, T, V, H, W]
  - 转换为extrinsics和intrinsics
  ↓
主Forward（使用体素化）
  - Patch Embedding [B*T*V, 729, 1024]
  - ViewMixer [B, T, V, 729, 1024]（跨视角对齐）
  - Voxelization（每个时间步t）
    - 反投影 → 体素索引 → 加权聚合 → voxel_tokens
  - Padding和Token组装 [B*T, 1+4+max_N, 1024]
  - Time embedding注入
  - Transformer 24层（体素化路径路由）
    - Stage-1: 同ID时间窗口路由
    - Stage-2: 同ID时间注意力路由
    - Stage-3: 混合路由
  - 输出头
    - CameraHead: pose_enc [B, V, 9]
    - DepthHead: depth [B, T, V, H, W]
    - PointHead: world_points [B, T, V, 3, H, W]
    - VoxelGaussianHead: teacher_gaussian_params [B, T, N_voxels, 83]
    - FusionHead + FusedGaussianHead: fused_gaussian_params [B, T, N_t, 83]
    - GaussianRenderer: rendered_images [B, T, V, 3, H, W]
```

### 5.2 关键reshape操作（体素化路径）

#### Stage-1 同ID时间窗口路由
```python
# 体素化路径：使用体素ID进行跨时间步对齐
tokens: [B*T, P, 1024]
  ↓ reshape
[B, T, P, 1024]
  ↓ 使用voxel_ids进行分组
# 按体素ID分组，同一ID的体素在不同时间步间attention
tokens_out: [B, T, P, 1024]
  ↓ reshape back
[B*T, P, 1024]
```

#### Stage-2 同ID时间注意力路由
```python
# 体素化路径：时序路由
tokens: [B*T, P, 1024]
  ↓ reshape
[B, T, P, 1024]
  ↓ 使用voxel_ids进行时序对齐
# 按体素ID分组，跨时间步时序attention
tokens_out: [B, T, P, 1024]
```

---

## 四、训练策略与损失函数

### 4.1 参数冻结策略（Stage 1）

**冻结**：
- Patch embed (DINOv2 encoder)
- Stage-1 (0-7层) 和 Stage-3 (18-23层) 的Transformer blocks
- CameraHead/DepthHead/PointHead的主体权重

**训练**：
- ViewMixer (Stage-0)
- Stage-2 (8-17层)
- Camera mask gate (γ)
- Camera/Register tokens
- Time/View/Camera embeddings
- Spatial mask head

### 4.2 损失函数

#### Multi-view Consistency Loss
```python
loss = (
    1.0 * depth_loss +           # 深度一致性
    0.5 * reproj_loss +          # 重投影一致性
    0.1 * smoothness_loss +       # 几何平滑性
    0.1 * conf_loss +             # 置信度正则化
    0.5 * mask_loss               # SegAnyMo mask监督
)
```

#### Dual Stream Separation Loss
- 鼓励位姿流和几何流特征分离

#### 体素化路径损失（如果启用）

**1. Teacher-Student蒸馏损失**：
- Teacher：每个视角独立预测的高斯参数（VoxelGaussianHead）
- Student：融合后的统一高斯参数（FusedGaussianHead）
- 损失：KL散度或MSE损失

**2. 渲染损失**：
- 渲染图像与真实图像的重建损失
- `render_loss = L1(rendered_images, gt_images)`

**3. 时序一致性损失**：
- 使用TemporalTracker建立跨时间步关联
- 鼓励相同体素ID在不同时间步的特征一致性

**4. 静态/动态分离损失**：
- 使用static_mask分离静态和动态高斯
- 静态高斯应该保持稳定，动态高斯可以变化

---

## 六、完整前向传播流程（体素化模式）

### 6.1 体素化路径完整流程

```
输入: [B, T, V, 3, H, W]
  ↓
1. Patch Embedding
  [B*T*V, 729, 1024]
  ↓
2. ViewMixer (Stage-0)
  [B, T, V, 729, 1024] → [B, T, V, 729, 1024]
  ↓
3. 第一次Forward（获取深度和相机参数）
  - 使用非体素化路径快速预测
  - depth: [B, T, V, H, W]
  - pose_enc: [B, V, 9] → extrinsics, intrinsics
  ↓
4. Voxelization (每个时间步t)
  For t in range(T):
    - patch_tokens_t: [B, V, 729, 1024]
    - depth_t: [B, V, H, W]
    - intrinsics_t: [B, V, 3, 3]
    - extrinsics_t: [B, V, 3, 4]
    ↓
    VoxelizationModule:
      - 反投影 → points3d: [B*V*N, 3]
      - 体素索引 → voxel_ids: [N_voxels]
      - 加权聚合 → voxel_feat: [N_voxels, 1024]
      - 生成tokens → voxel_tokens: [N_voxels, 1024]
    ↓
    voxel_tokens_list[t] = List of [N_t, 1024] per batch
  ↓
5. Padding和Token组装
  - Padding到统一长度max_N
  - voxel_tokens: [B, T, max_N, 1024]
  - 添加camera/register tokens: [B*T, 1+4+max_N, 1024]
  ↓
6. Transformer主网络（24层）
  - 使用体素化路径的特殊路由策略
  - 输出: [B*T, P, 2048] × 24层
  ↓
7. FusionHead (多视角融合)
  - 输入: voxel_tokens [B, T, V, N_t, 2048] 或 [B, T, N_t, 2048]
  - 融合: [B, T, N_t, 256]
  - 输出: G_t_full [B, T, N_t, 83]
  ↓
8. FusedGaussianHead
  - 输入: fused_features [B, T, N_t, 256]
  - 输出: gaussian_params [B, T, N_t, 83]
  ↓
9. GaussianRenderer
  - 输入: gaussian_params [B, T, N_t, 83]
  - 输出: rendered_images [B, T, V, 3, H, W]
  ↓
10. 其他输出头（Teacher端）
  - VoxelGaussianHead: 每个视角独立预测
  - CameraHead: 相机参数（从第一次forward获取）
  - DepthHead: 深度图（从第一次forward获取）
```

### 6.2 关键设计决策

1. **两阶段Forward**：
   - 第一次：快速获取深度和相机参数（不使用体素化）
   - 第二次：使用体素化进行详细预测

2. **体素化时机**：
   - 在ViewMixer之后，Transformer之前
   - 每个时间步独立体素化，保持时序信息

3. **路由策略适配**：
   - 体素化路径下，Stage-1使用"同ID时间窗口"路由
   - Stage-2使用"同ID时间注意力"路由
   - 利用体素ID进行跨时间步对齐

4. **内存优化**：
   - 体素数可能不同，使用padding统一处理
   - 支持变长序列处理（如果实现）

---

## 七、模块实现细节

### 7.1 ViewMixer实现细节

#### LoRALinear实现
```python
class LoRALinear(nn.Module):
    def __init__(self, in_features, out_features, rank=16, alpha=1.0):
        self.lora_A = nn.Parameter(torch.zeros(rank, in_features))
        self.lora_B = nn.Parameter(torch.zeros(out_features, rank))
        # 初始化: A使用kaiming_uniform, B初始化为0
    
    def forward(self, x):
        # LoRA: delta = (x @ A^T) @ B^T
        lora_output = (x @ self.lora_A.transpose(0, 1)) @ self.lora_B.transpose(0, 1)
        return self.alpha * lora_output
```

#### 参数量分析
- 标准Linear: `1024 × 1024 = 1,048,576` 参数
- LoRA (rank=16): `16 × 1024 + 1024 × 16 = 32,768` 参数
- **参数量减少**: 约97%的参数量节省

### 7.2 体素化实现细节

#### 体素索引计算
```python
# 自适应体素大小
scene_extent = (points3d.max(dim=0)[0] - points3d.min(dim=0)[0]).max()
voxel_size = scene_extent / (target_num_voxels ** (1/3))

# 体素索引
ix = (points3d[:, 0] / voxel_size).floor().long()
iy = (points3d[:, 1] / voxel_size).floor().long()
iz = (points3d[:, 2] / voxel_size).floor().long()

# 体素ID（Morton编码或简单索引）
voxel_id = ix * grid_size_y * grid_size_z + iy * grid_size_z + iz
```

#### 加权聚合
```python
# 置信度加权
conf_weights = F.softmax(confidence, dim=0)  # [N]
# 加权聚合
voxel_feat = scatter_add(
    patch_feat * conf_weights.unsqueeze(-1),
    voxel_indices,
    dim_size=num_voxels
)
```

### 7.3 CameraHead多视角适配

#### View-Specific Projection
```python
# 每个视角使用独立投影
for v_idx in range(V):
    pose_tokens_v = pose_tokens_mv[:, v_idx:v_idx+1, :]  # [B, 1, C]
    pose_tokens_projected.append(self.view_proj[v_idx](pose_tokens_v))

# 确保视角差异
pose_tokens_mv = torch.cat(pose_tokens_projected, dim=1)  # [B, V, C]
```

#### 迭代细化
```python
# 每个视角独立迭代
for v_idx in range(V):
    pose_tokens_v = pose_tokens_mv[:, v_idx:v_idx+1, :]
    pred_pose_enc_list_v = self.trunk_fn(pose_tokens_v, num_iterations=4)
    # 每个视角得到独立的pose_enc [B, 1, 9]
```

---

## 八、总结

### 8.1 核心创新点

1. **ViewMixer (Stage-0)**：轻量级跨视角对齐，LoRA设计几乎零侵入
2. **三阶段路由策略**：Stage-1多视角聚合 → Stage-2时序建模 → Stage-3混合融合
3. **体素化路径**：将多视角特征体素化到统一世界网格，支持更好的几何一致性
4. **多视角融合**：FusionHead实现V→1统一表示，支持静态/动态分离
5. **可微渲染**：使用GaussianRenderer将融合后的高斯参数渲染回各视角

### 8.2 架构优势

1. **模块化设计**：体素化、融合、渲染都是可选模块，可以灵活组合
2. **内存效率**：LoRA设计、体素化稀疏表示、变长序列支持
3. **多任务支持**：同时支持相机估计、深度预测、点云生成、高斯渲染
4. **向后兼容**：支持legacy单视角时序模式

### 8.3 未来改进方向

1. **稀疏3D U-Net**：在体素化路径中使用稀疏卷积
2. **几何引导聚合**：ViewMixer的可选几何引导模式
3. **时序一致性优化**：更强大的TemporalTracker
4. **动态体素大小**：根据场景复杂度自适应调整

---