# run_fig1_auto.py 代码分析（中文）

## 一、概述

`run_fig1_auto.py` 是一个用于处理动态场景图像的脚本，使用了两个深度学习模型：
1. **DPG模型** (来自 `vggt_t_mask_mlp_fin10`) - 自定义改进版本
2. **VGGT模型** (来自 `vggt`) - 原始版本

该脚本对图像序列进行推理，预测深度图、3D点云和相机参数。

---

## 二、输入输出分析

### 2.1 输入

#### 2.1.1 图像输入
- **格式**: PNG或JPG图像文件路径列表
- **预处理**: 
  - 通过 `load_and_preprocess_images()` 函数处理
  - 图像归一化到 [0, 1] 范围
  - 尺寸调整为可被14整除（patch_size=14）
  - 默认宽度518px，高度按比例调整
- **最终形状**: `(N, 3, H, W)`
  - N: 图像数量（最多24张）
  - 3: RGB通道
  - H, W: 高度和宽度

#### 2.1.2 数据目录结构
```
online_img3/
  └── {category}/  (例如: "1/")
      ├── 0000.png
      ├── 0001.png
      └── ...
```

### 2.2 输出

#### 2.2.1 点云文件 (.ply)
- **位置**: `{data_name}/fig1_update_{model_name}/{model_name}_{category}/`
- **格式**: PLY格式点云文件
- **内容**: 
  - 3D世界坐标点（通过深度图反投影得到）
  - RGB颜色信息（来自原始图像）
  - 文件命名: `{num}_dep.ply`

#### 2.2.2 相机参数文件 (.json)
- **外参文件** (`{num_name}_ext.json`):
  - 包含每帧的相机外参矩阵 (3x4)
  - 格式: `[R|t]`，其中R是3x3旋转矩阵，t是3x1平移向量
- **内参文件** (`{num_name}_int.json`):
  - 包含每帧的相机内参矩阵 (3x3)
  - 格式: `[[fx, 0, cx], [0, fy, cy], [0, 0, 1]]`

#### 2.2.3 可视化图像
- **图像文件**: `{num_name}_images.png`
- **内容**: 处理后的输入图像（保存为可视化参考）

---

## 三、网络结构分析（递归）

### 3.1 整体架构

```
输入图像序列 (N, 3, H, W)
    ↓
[Aggregator] - 特征聚合模块
    ↓
[Camera Head] - 相机参数预测
[Depth Head] - 深度图预测  
[Point Head] - 3D点预测
    ↓
深度图 + 相机参数
    ↓
[backproject_depth_to_points_batch] - 深度反投影
    ↓
输出: 点云 + 相机参数
```

### 3.2 Aggregator（聚合器）模块

#### 3.2.1 功能
将多帧图像转换为统一的token表示，使用交替注意力机制融合时空信息。

#### 3.2.2 架构层次

**A. Patch Embedding层**
- **类型**: DINOv2 Vision Transformer (ViT-Large)
- **配置**: 
  - `img_size=518`
  - `patch_size=14`
  - `embed_dim=1024`
- **输出**: Patch tokens `(B*S, P, 1024)`
  - B: batch size
  - S: 序列长度（帧数）
  - P: patch数量

**B. 特殊Tokens**
- **Camera Token**: `(1, 2, 1, 1024)`
  - 第一帧使用索引0，其余帧使用索引1
- **Register Token**: `(1, 2, 4, 1024)`
  - 4个register tokens，同样分第一帧和其余帧
- **拼接后**: `(B*S, 1+4+P, 1024)`

**C. 位置编码**
- **类型**: Rotary Position Embedding (RoPE)
- **频率**: 100 (rope_freq=100)
- **作用**: 为patch tokens提供2D位置信息

**D. 交替注意力块（Alternating Attention）**
- **两种注意力模式**:
  1. **Frame Attention** (`frame_blocks`): 
     - 在单帧内部进行自注意力
     - Token形状: `(B*S, P, C)`
     - 24个Block
  2. **Global Attention** (`global_blocks`):
     - 跨帧全局注意力
     - Token形状: `(B, S*P, C)`
     - 24个Block
- **交替顺序**: `["frame", "global"]`
- **总深度**: 24层（frame + global = 48个blocks）

**E. 输出**
- 每个Block输出经过拼接: `(B, S, P, 2048)` 
  - 2048 = 1024 (frame) + 1024 (global)
- 返回所有层的中间特征列表 + `patch_start_idx=5`（相机token(1) + register tokens(4)）

#### 3.2.3 关键参数
```python
embed_dim = 1024
depth = 24
num_heads = 16
mlp_ratio = 4.0
num_register_tokens = 4
patch_size = 14
```

### 3.3 Camera Head（相机头）模块

#### 3.3.1 功能
从聚合的tokens预测相机姿态编码（pose encoding），包括：
- 绝对平移 T (3维)
- 旋转四元数 quat (4维)
- 视场角 FoV (2维)
- **总计**: 9维姿态编码

#### 3.3.2 架构

**A. 输入处理**
- 提取camera token: `aggregated_tokens_list[-1][:, :, 0]`
- 形状: `(B, S, 1024)`

**B. Trunk网络**
- 4层Transformer Block
- 输入维度: 2048 (来自frame+global拼接)
- 使用LayerNorm归一化

**C. 迭代细化机制**
- **迭代次数**: 4次
- **流程**:
  1. 第一迭代：使用可学习的空pose token
  2. 后续迭代：基于前一次预测的pose encoding
  3. Adaptive Layer Normalization (AdaLN) 调制
  4. Trunk处理
  5. MLP输出delta更新
  6. 累加得到新的pose encoding

**D. 激活函数**
- Translation: linear
- Quaternion: linear (归一化为单位四元数)
- FOV: ReLU (确保正值)

**E. 输出转换**
- `pose_encoding_to_extri_intri()` 将9维编码转换为：
  - 外参矩阵 `(3, 4)`: `[R|t]`
  - 内参矩阵 `(3, 3)`: `[[fx, 0, cx], [0, fy, cy], [0, 0, 1]]`

### 3.4 Depth Head（深度头）模块

#### 3.4.1 功能
预测每像素的深度值及其置信度。

#### 3.4.2 架构（DPT Head）

**A. 多尺度特征提取**
- 使用4个中间层的特征:
  - Layer 4, 11, 17, 23
- 每个层投影到: `[256, 512, 1024, 1024]` 通道

**B. 特征融合网络（Scratch）**
- **4个RefineNet块**进行特征融合:
  - `refinenet4`: 最高层特征（无残差）
  - `refinenet3`: 融合layer3
  - `refinenet2`: 融合layer2
  - `refinenet1`: 融合layer1
- **上采样/下采样**:
  - Layer1: 4x上采样（转置卷积）
  - Layer2: 2x上采样（转置卷积）
  - Layer3: 保持
  - Layer4: 2x下采样（卷积）

**C. 输出层**
- `output_conv1`: 256 → 128通道
- `output_conv2`: 128 → 32 → 2通道
  - 2通道: [深度值, 置信度]

**D. 激活函数**
- 深度值: `exp` 激活（确保正值）
- 置信度: `expp1` 激活（exp + 1）

**E. 输出形状**
- 深度图: `(B, S, 1, H, W)`
- 置信度: `(B, S, 1, H, W)`

### 3.5 Point Head（点云头）模块

#### 3.5.1 功能
直接预测每像素的3D世界坐标点。

#### 3.5.2 架构
- **与Depth Head相同的DPT架构**
- **输出维度**: 4通道
  - 3通道: 3D坐标 (x, y, z)
  - 1通道: 置信度
- **激活函数**:
  - 坐标: `inv_log` 激活（逆对数）
  - 置信度: `expp1`

**注意**: 在 `run_fig1_auto.py` 中，使用的是深度图反投影方法，而不是直接使用point_head的输出。

### 3.6 深度反投影函数

#### 3.6.1 `backproject_depth_to_points_batch()`
将深度图转换为3D世界坐标点。

**步骤**:
1. **创建像素网格**: `(u, v, 1)` 形状 `(H, W, 3)`
2. **逆内参变换**: `K^-1 @ pixels` → 相机坐标系
3. **深度加权**: `depth * cam_coords`
4. **外参变换**: `T_wc^-1 @ cam_points_h` → 世界坐标系

**数学公式**:
```
cam_coords = K^-1 @ pixels
world_points = T_wc^-1 @ (depth * cam_coords)
```

---

## 四、处理流程详解

### 4.1 主流程 (`process` 函数)

```
1. 加载图像 → load_and_preprocess_images()
   ↓
2. 模型推理 (无梯度)
   ├─ aggregator → aggregated_tokens_list, ps_idx
   ├─ camera_head → pose_enc → extrinsic, intrinsic
   ├─ depth_head → depth_map, depth_conf
   └─ point_head → point_map, point_conf (未使用)
   ↓
3. 深度反投影 → backproject_depth_to_points_batch()
   ↓
4. 保存结果
   ├─ 点云文件 (.ply)
   ├─ 相机参数 (.json)
   └─ 可视化图像 (.png)
```

### 4.2 主函数流程 (`__main__`)

```
1. 初始化设备 (CUDA/CPU)
   ↓
2. 加载两个模型
   ├─ DPG: 从checkpoint加载 ("checkpoint/checkpoint_150.pt")
   └─ VGGT: 从HuggingFace加载 ("facebook/VGGT-1B")
   ↓
3. 遍历数据目录
   ├─ 找到所有category文件夹
   ├─ 读取图像文件（排序，最多24张）
   └─ 对每个模型分别推理
       ├─ DPG模型 → fig1_update_dpg/
       └─ VGGT模型 → fig1_update_vggt/
```

---

## 五、关键数据流

### 5.1 数据形状变化

```
输入图像: (N, 3, H, W)
    ↓ unsqueeze(0)
(B=1, N, 3, H, W)
    ↓ aggregator
aggregated_tokens_list: 
  - 24层特征，每层 (B, N, P, 2048)
    ↓ camera_head
pose_enc: (B, N, 9)
    ↓ pose_encoding_to_extri_intri
extrinsic: (B, N, 3, 4)
intrinsic: (B, N, 3, 3)
    ↓ depth_head
depth_map: (B, N, H, W)
    ↓ backproject_depth_to_points_batch
world_points: (B, N, H*W, 3)
```

### 5.2 Token索引

- `patch_start_idx = 5`
  - 索引0: Camera Token
  - 索引1-4: Register Tokens
  - 索引5+: Patch Tokens

---

## 六、两个模型的区别

### 6.1 DPG模型 (vggt_t_mask_mlp_fin10)
- **改进点**:
  - 支持temporal_features输入
  - 可能包含mask相关的MLP模块
  - 从checkpoint加载（自定义训练）

### 6.2 VGGT模型 (vggt)
- **原始版本**:
  - 从HuggingFace预训练模型加载
  - 标准VGGT架构

---

## 七、技术细节

### 7.1 混合精度推理
- 使用 `torch.cuda.amp.autocast(dtype=bfloat16/float16)`
- 取决于GPU能力（compute capability >= 8 使用bfloat16）

### 7.2 内存优化
- 点云采样: `world_points[num][::2]` (每隔一个点采样)
- 推理时关闭梯度: `torch.no_grad()`

### 7.3 坐标系
- **OpenCV坐标系**:
  - x: 右
  - y: 下
  - z: 前

---

## 八、总结

该脚本实现了一个完整的视觉几何推理管道：

1. **输入**: 多帧图像序列
2. **处理**: 
   - 特征提取（ViT + 交替注意力）
   - 相机姿态估计（迭代细化）
   - 深度预测（DPT架构）
3. **输出**: 
   - 3D点云
   - 相机内外参
   - 可视化结果

核心创新点在于使用**交替注意力机制**（frame attention + global attention）来融合多帧信息，从而同时预测深度、3D点和相机参数。


